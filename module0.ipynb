{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed78776",
   "metadata": {},
   "source": [
    "text1 = \"The cat chased the dog\"\n",
    "text2 = \"The dog chased the cat\"\n",
    "\n",
    "text = \"The capital of France is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ac1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2545afe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'chased', 'the', 'dog']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"The cat chased the dog\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27be1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 10, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tokenize2(text):\n",
    "    parts = text.split()\n",
    "    ids = []\n",
    "    for part in parts:\n",
    "        if part in vocab:\n",
    "            value = vocab[part]\n",
    "        else:\n",
    "            value = vocab[\"<unk>\"]\n",
    "        ids.append(value)\n",
    "    return ids\n",
    "\n",
    "tokenize2(\"The cat chased the dog\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a12dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {\n",
    "    0: \"The\",\n",
    "    1: \"cat\",\n",
    "    2: \"dog\",\n",
    "    3: \"chased\",\n",
    "    4: \"capital\",\n",
    "    5: \"of\",\n",
    "    6: \"France\",\n",
    "    7: \"is\",\n",
    "    8: \"<unk>\",\n",
    "    9: \"the\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06713d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(ids):\n",
    "    text = \"\"\n",
    "    for id in ids:\n",
    "        part = reverse_vocab[id]\n",
    "        text += part + \" \"\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ddbd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [464, 3290, 26172, 262, 3797]\n",
      "Decoded text: The dog chased the cat\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokens = enc.encode(\"The dog chased the cat\")\n",
    "text = enc.decode(tokens)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded text:\", text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d6278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 3290, 26172, 262, 3797]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "gpt2_ids = enc.encode(\"The dog chased the cat\")\n",
    "gpt2_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e02f4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog chased the cat'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(gpt2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29f340cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1997fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"The cat chased the dog\"\n",
    "text2 = \"The dog chased the cat\"\n",
    "\n",
    "text = \"The capital of France is\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e5799f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[976, 9059, 135896, 290, 6446]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o200_ids = enc.encode(text1)\n",
    "o200_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f37b10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f243b95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-27b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "981c17a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 818, 5866, 83755, 506, 4799]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.encode(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94a527e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vocabulary path (gemma_tokenizer) should be a directory\n"
     ]
    }
   ],
   "source": [
    "processor.tokenizer.save_vocabulary(\"gemma_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49311685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany. rome is in italy, \\n\\nmadrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united states is not berlin. \\nalthough these places are often mentioned together, although these capitals are often mentioned together, although these are often mentioned together, \\neach country has its own capital, and each country has its own city, and each capital has its own identity, and each capital has its own history. washington \\nis the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, and berlin is known for art and \\nhistory, and rome is known for art and history, and madrid is known for culture and history, and lisbon is known for culture and art. rome is rich with culture, \\nrome is rich with history, rome is rich with art, and madrid is rich with art and culture. lisbon is a unique city in portugal with a rich history, a rich culture, \\nand a rich identity. these capitals are often mentioned together, these capitals are often mentioned together in art, these capitals are often mentioned together \\nin culture, these capitals are often mentioned together in history. the united states is not in europe, the united states is not in any european place, and \\nwashington is not in any european city. each european country is made of important capitals, and each european capital is made of art, history, and culture. \\nthe capital of the united states is washington, the capital of the united kingdom is london, the capital of france is paris, the capital of germany is berlin, \\nthe capital of italy is rome, the capital of spain is madrid, and the capital of portugal is lisbon. while these capitals are in europe, while these capitals are \\nin europe, washington is in the united states. these capitals remain important, these remain important, these places remain important in the world. the \\ncapital of each country is its own, the capital of each country is its identity, the capital of each country is its culture. europe is made of many, \\neurope is made of many capitals, europe is made of many important places. each place is rich with culture, each place is rich with history, and each capital is \\n\\nrich with identity. the world is made of capitals, the world is made of, the world is made of places, and the capital of the united states is washington, \\nnot any european city, not paris, not london, not berlin. the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany.\\nrome is in italy, madrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united \\nstates is not berlin. although these places are often mentioned together, each country has its own capital, and each capital has its own identity. \\nwashington is the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, while berlin is \\nfamous for its culture and history. rome is rich with history, and madrid is known for its art and culture. lisbon is a unique city in portugal \\nwith a rich history. these capitals are often mentioned together, although each place with its own culture. the united states is not in europe, \\nand washington is not in any european country. these european capitals are made of history, culture, and identity. each country in europe has a capital, \\nand each capital is known for important. london, paris, berlin, rome, madrid, and lisbon remain important places in the world. while these capitals\\nare in europe, washington is in the united states. although these places are not in the country, they are often mentioned together in art, culture, \\nand history. the capital of each country is its own. europe is made of many capitals, and each has a capital with a unique history. \\nthe world is of important places, and the capital of the united states is washington, not any european city.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (\"text.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"tokenizer.json\")\n",
    "\n",
    "tokenizer.encode(\"states\"), tokenizer.decode([4, 58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94218396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9691455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=\"text.txt\",\n",
    "    model_prefix=\"spm_tokenizer\",\n",
    "    vocab_size=64,\n",
    "    model_type=\"bpe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f29fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b6730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a82f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befcc80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70bbd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(vocab_size=64, special_tokens=[\"<unk>\"])\n",
    "\n",
    "hf_tokenizer.train([\"text.txt\"], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4faff4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, [10, 7, 6, 16, 9, 5, 10, 49, 44, 28, 5, 3, 21])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.get_vocab_size(), hf_tokenizer.encode(\"The dog chased the cat\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f5aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.save(\"hf_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77266437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h e c a t s a t on the ma t'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"hf_tokenizer.json\")\n",
    "\n",
    "fast_tokenizer.encode(\"The cat sat on the mat\")\n",
    "\n",
    "fast_tokenizer.decode(fast_tokenizer.encode(\"The cat sat on the mat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2a8812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GorkemParadise/hf_tokenizer/commit/8d745a8a3a5ddf53207c1edb887e7f8d3f16b174', commit_message='Upload tokenizer', commit_description='', oid='8d745a8a3a5ddf53207c1edb887e7f8d3f16b174', pr_url=None, repo_url=RepoUrl('https://huggingface.co/GorkemParadise/hf_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='GorkemParadise/hf_tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.push_to_hub(\"GorkemParadise/hf_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

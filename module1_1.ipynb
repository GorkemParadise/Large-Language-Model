{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f21bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"the capital of the united states is not\"\n",
    "\n",
    "next_token = \" \"\n",
    "\n",
    "output = \"the capital of france is paris,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5804f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"the capital <pad> <pad> <pad> <pad> \"\n",
    "hedef = \"capital of <pad> <pad> <pad> <pad> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32fd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 12 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b34d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, [0, 61, 1, 61, 2, 61, 0, 61, 3, 61, 4, 58, 61, 5, 61, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"tokenizer.json\")\n",
    "\n",
    "ids = tokenizer.encode(prompt)\n",
    "\n",
    "len(ids), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6334f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 1437, 5279, 529, 506, 26974, 5022, 563, 711]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "gemma_model = Gemma3ForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "\n",
    "gemma_tokenizer.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156402e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbbc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "girdi = \"the letter capital of the united states is not\"\n",
    "cikti = \"letter capital of the united states is not London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f40a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany. rome is in italy, \\n\\nmadrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united states is not berlin. \\nalthough these places are often mentioned together, although these capitals are often mentioned together, although these are often mentioned together, \\neach country has its own capital, and each country has its own city, and each capital has its own identity, and each capital has its own history. washington \\nis the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, and berlin is known for art and \\nhistory, and rome is known for art and history, and madrid is known for culture and history, and lisbon is known for culture and art. rome is rich with culture, \\nrome is rich with history, rome is rich with art, and madrid is rich with art and culture. lisbon is a unique city in portugal with a rich history, a rich culture, \\nand a rich identity. these capitals are often mentioned together, these capitals are often mentioned together in art, these capitals are often mentioned together \\nin culture, these capitals are often mentioned together in history. the united states is not in europe, the united states is not in any european place, and \\nwashington is not in any european city. each european country is made of important capitals, and each european capital is made of art, history, and culture. \\nthe capital of the united states is washington, the capital of the united kingdom is london, the capital of france is paris, the capital of germany is berlin, \\nthe capital of italy is rome, the capital of spain is madrid, and the capital of portugal is lisbon. while these capitals are in europe, while these capitals are \\nin europe, washington is in the united states. these capitals remain important, these remain important, these places remain important in the world. the \\ncapital of each country is its own, the capital of each country is its identity, the capital of each country is its culture. europe is made of many, \\neurope is made of many capitals, europe is made of many important places. each place is rich with culture, each place is rich with history, and each capital is \\n\\nrich with identity. the world is made of capitals, the world is made of, the world is made of places, and the capital of the united states is washington, \\nnot any european city, not paris, not london, not berlin. the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany.\\nrome is in italy, madrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united \\nstates is not berlin. although these places are often mentioned together, each country has its own capital, and each capital has its own identity. \\nwashington is the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, while berlin is \\nfamous for its culture and history. rome is rich with history, and madrid is known for its art and culture. lisbon is a unique city in portugal \\nwith a rich history. these capitals are often mentioned together, although each place with its own culture. the united states is not in europe, \\nand washington is not in any european country. these european capitals are made of history, culture, and identity. each country in europe has a capital, \\nand each capital is known for important. london, paris, berlin, rome, madrid, and lisbon remain important places in the world. while these capitals\\nare in europe, washington is in the united states. although these places are not in the country, they are often mentioned together in art, culture, \\nand history. the capital of each country is its own. europe is made of many capitals, and each has a capital with a unique history. \\nthe world is of important places, and the capital of the united states is washington, not any european city.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"text.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f137bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f2a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_dataset import create_data_loader\n",
    "\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "ids_text = \"\"\n",
    "\n",
    "for token_id in token_ids:\n",
    "    ids_text += f\"{token_id}\"\n",
    "with open(\"token_ids.txt\", \"w\") as f:\n",
    "    f.write(ids_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17628d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride = 12\n",
    "\n",
    "train_data_loader = create_data_loader(token_ids, context_length, stride, 1, False)\n",
    "\n",
    "len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea95ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [tensor([[ 0, 61,  1, 61,  2, 61,  0, 61,  3, 61,  4, 58]]), tensor([[61,  1, 61,  2, 61,  0, 61,  3, 61,  4, 58, 61]])])\n",
      "(1, [tensor([[61,  5, 61,  6, 61,  7, 59, 61,  0, 61,  1, 61]]), tensor([[ 5, 61,  6, 61,  7, 59, 61,  0, 61,  1, 61,  2]])])\n",
      "(2, [tensor([[ 2, 61,  8, 61,  5, 61,  9, 60, 61, 10, 61, 11]]), tensor([[61,  8, 61,  5, 61,  9, 60, 61, 10, 61, 11, 61]])])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in enumerate(train_data_loader):\n",
    "    print(batch)\n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e5eb1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Embeeding yani Sözlük Anlamlarının Sayısal Değerleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e94581",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_1 = 0.1 #parlaklık ekseni\n",
    "the_2 = 0.4 #sertlik ekseni\n",
    "the_3 = 0.0 #mavilik ekseni\n",
    "the_4 = 0.0 #kırmızılık ekseni\n",
    "the_5 = 0.7 #yeşillik ekseni\n",
    "\n",
    "capital_1 = 0.0 #parlaklık ekseni\n",
    "capital_2 = 0.2 #sertlik ekseni\n",
    "capital_3 = 0.24 #mavilik ekseni\n",
    "capital_4 = 0.3 #kırmızılık ekseni\n",
    "capital_5 = 0.1 #yeşillik ekseni\n",
    "#Kelimeler 5 boyutlu uzayda incelenir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cbb17e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.1, 0.0, 0.0, 0.0],\n",
       " [0.4, 0.2, 0.04, 0.3],\n",
       " [0.0, 0.24, 0.02, 0.01],\n",
       " [0.0, 0.0, 0.03, 0.01],\n",
       " [0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dict_v2 = {\n",
    "    \"the\": [0.1, 0.0, 0.0, 0.0],\n",
    "    \"capital\": [0.4, 0.2, 0.04, 0.3],\n",
    "    \"of\": [0.0, 0.24, 0.02, 0.01],\n",
    "    \"united\": [0.0, 0.0, 0.03, 0.01],\n",
    "    \"states\": [0.0, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "dict_v2[\"the\"], dict_v2[\"capital\"], dict_v2[\"of\"], dict_v2[\"united\"], dict_v2[\"states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31744d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "data = [\n",
    "    go.Scatter3d(\n",
    "        x=[0.1, 0.4, 0.0, 0.0, 0.0],\n",
    "        y=[0.0, 0.2, 0.24, 0.0, 0.0],\n",
    "        z=[0.0, 0.0, 0.0, 0.03, 0.0],\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=\"red\",\n",
    "        ),\n",
    "        text=[\"the\", \"capital\", \"of\", \"united\", \"states\"],\n",
    "        hoverinfo=\"text\"\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Sertlik\",\n",
    "        yaxis_title=\"Parlaklık\",\n",
    "        zaxis_title=\"Kırmızılık\",\n",
    "    ),\n",
    "    title=\"Sözlük V1\",\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "pyo.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40229c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "def plot_dots(sentences_data, title):\n",
    "    data = [\n",
    "        go.Scatter3d(\n",
    "            x=sentence_data[\"words\"][:, 0],\n",
    "            y=sentence_data[\"words\"][:, 1],\n",
    "            z=sentence_data[\"words\"][:, 2],\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=sentence_data[\"color\"],\n",
    "            ),\n",
    "            text=sentence_data[\"labels\"],\n",
    "            hoverinfo=\"text\",\n",
    "        )   for sentence_data in sentences_data\n",
    "    ]\n",
    "\n",
    "\n",
    "    layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Sertlik\",\n",
    "        yaxis_title=\"Parlaklık\",\n",
    "        zaxis_title=\"Kırmızılık\",\n",
    "        ),\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    pyo.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deb485b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentences = [\n",
    "    {\n",
    "        \"words\": np.array([\n",
    "        [0.1, 0.0, 0.0, 0.0],\n",
    "        [0.4, 0.2, 0.04, 0.3],\n",
    "        [0.0, 0.24, 0.02, 0.01],\n",
    "        [0.0, 0.0, 0.03, 0.01],\n",
    "        [0.0, 0.0, 0.0, 0.0]\n",
    "        ]),\n",
    "        \"labels\": [\"the\", \"capital\", \"of\", \"united\", \"states\"],\n",
    "        \"color\": \"red\",\n",
    "    }\n",
    "]\n",
    "\n",
    "plot_dots(sentences, \"Sözlük V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6312a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embeddings = torch.nn.Embedding(num_embeddings=64, embedding_dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78cde2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the capital of united states and the capital of france\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "873a5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 61, 1, 61, 2, 61, 3, 61, 4, 58, 61, 10, 61, 0, 61, 1, 61, 2, 61, 8]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d2a5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2027e-03, -8.9293e-01, -3.5829e-01,  4.8924e-01],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [-5.0212e-01, -7.1274e-01, -1.1834e-01,  1.3504e+00],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [ 1.5401e+00, -9.7600e-01, -9.6394e-01,  1.7890e+00],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [ 2.6744e-01,  6.5139e-01, -2.6714e-01,  1.0090e-01],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [ 6.7365e-01,  2.8220e+00,  1.1312e+00,  1.0759e+00],\n",
       "        [ 9.9578e-01,  1.2575e+00,  2.1137e+00, -5.1294e-01],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [ 4.0406e-01,  1.8723e-01, -1.4722e+00,  1.0226e+00],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [ 2.2027e-03, -8.9293e-01, -3.5829e-01,  4.8924e-01],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [-5.0212e-01, -7.1274e-01, -1.1834e-01,  1.3504e+00],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [ 1.5401e+00, -9.7600e-01, -9.6394e-01,  1.7890e+00],\n",
       "        [-2.6727e+00,  3.9303e-01, -6.2246e-01,  4.8237e-01],\n",
       "        [-1.5784e-01,  9.2165e-01,  9.3989e-01, -5.6740e-01]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanings = embeddings(torch.tensor(tokens))\n",
    "meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a079051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1437, 5279, 529, 26974, 5022, 532, 506, 5279, 529, 61034]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_tokens = gemma_tokenizer.encode(sentence)\n",
    "gemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a745da8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1388, -0.2165, -0.5386,  ..., -0.2465,  0.0267,  0.1258],\n",
       "        [-0.1668,  1.2761,  0.7914,  ...,  0.6546,  1.1767, -0.4889],\n",
       "        [ 0.8079,  0.4723,  0.5718,  ...,  0.5138, -0.3190, -0.2579],\n",
       "        ...,\n",
       "        [ 0.8079,  0.4723,  0.5718,  ...,  0.5138, -0.3190, -0.2579],\n",
       "        [ 0.3812,  0.2621,  0.0798,  ...,  0.0192,  0.0074,  0.5883],\n",
       "        [ 0.1823,  1.0524,  0.0210,  ..., -0.2009, -0.4765, -1.7650]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.model.embed_tokens(torch.tensor(gemma_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "300ecb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1152])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_meanings = gemma_model.model.embed_tokens(torch.tensor(gemma_tokens))\n",
    "gemma_meanings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ac2a44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " '▁capital',\n",
       " '▁of',\n",
       " '▁united',\n",
       " '▁states',\n",
       " '▁and',\n",
       " '▁the',\n",
       " '▁capital',\n",
       " '▁of',\n",
       " '▁france']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fba778c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "gemma_sentences = [\n",
    "  {\n",
    "    \"words\": gemma_meanings.detach().numpy(),\n",
    "    \"labels\": gemma_tokenizer.tokenize(sentence),\n",
    "    \"color\": \"red\",\n",
    "  },\n",
    "]\n",
    "plot_dots(gemma_sentences, \"Gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3a878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
